# 🤖✨ Transformers

✅ En este módulo exploramos los **Transformers**, una de las arquitecturas más revolucionarias en el campo del Procesamiento de Lenguaje Natural (PLN), base de modelos como GPT.

---

## 🔍 Contenido

- ⚙️ Mecanismo de Self-Attention  
- 🧠 Arquitectura Transformer: Encoder y Decoder  
- 📚 Tokenización con modelos preentrenados  
- 🔁 Uso de modelos como BERT y GPT con Hugging Face  
- 🧪 Aplicación a clasificación, resumen, traducción y generación de texto  
- 📊 Evaluación y visualización de resultados

🐍 Desarrollado en **Python**, con herramientas como:  
`transformers` (Hugging Face), `torch`, `datasets`, `tokenizers`, `pandas`, `matplotlib`

---

## 🚀 Habilidades desarrolladas

- 🔍 Comprensión de la arquitectura Transformer y su impacto en el PLN  
- 🧠 Implementación de modelos preentrenados sobre tareas personalizadas  
- 🔧 Fine-tuning para casos reales  
- 📈 Interpretación de métricas y visualización de resultados del modelo

---

## 🎯 Objetivo general

Comprender y aplicar la arquitectura **Transformer** en tareas prácticas de Procesamiento de Lenguaje Natural, utilizando modelos de vanguardia mediante librerías modernas como Hugging Face.

