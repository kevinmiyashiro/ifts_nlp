# ğŸ¤–âœ¨ Transformers

âœ… En este mÃ³dulo exploramos los **Transformers**, una de las arquitecturas mÃ¡s revolucionarias en el campo del Procesamiento de Lenguaje Natural (PLN), base de modelos como GPT.

---

## ğŸ” Contenido

- âš™ï¸ Mecanismo de Self-Attention  
- ğŸ§  Arquitectura Transformer: Encoder y Decoder  
- ğŸ“š TokenizaciÃ³n con modelos preentrenados  
- ğŸ” Uso de modelos como BERT y GPT con Hugging Face  
- ğŸ§ª AplicaciÃ³n a clasificaciÃ³n, resumen, traducciÃ³n y generaciÃ³n de texto  
- ğŸ“Š EvaluaciÃ³n y visualizaciÃ³n de resultados

ğŸ Desarrollado en **Python**, con herramientas como:  
`transformers` (Hugging Face), `torch`, `datasets`, `tokenizers`, `pandas`, `matplotlib`

---

## ğŸš€ Habilidades desarrolladas

- ğŸ” ComprensiÃ³n de la arquitectura Transformer y su impacto en el PLN  
- ğŸ§  ImplementaciÃ³n de modelos preentrenados sobre tareas personalizadas  
- ğŸ”§ Fine-tuning para casos reales  
- ğŸ“ˆ InterpretaciÃ³n de mÃ©tricas y visualizaciÃ³n de resultados del modelo

---

## ğŸ¯ Objetivo general

Comprender y aplicar la arquitectura **Transformer** en tareas prÃ¡cticas de Procesamiento de Lenguaje Natural, utilizando modelos de vanguardia mediante librerÃ­as modernas como Hugging Face.

