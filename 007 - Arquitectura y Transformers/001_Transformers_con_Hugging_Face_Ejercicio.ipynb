{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Reflexión y discusión\n",
        "\n",
        "- ¿Cuál de estos pipelines te pareció más sorprendente o útil?\n",
        "- ¿Creés que estas herramientas podrían usarse en un proyecto real? ¿En cuál?\n",
        "- ¿Notaste errores o sesgos? ¿Por qué creés que aparecen?\n",
        "\n"
      ],
      "metadata": {
        "id": "fH5RSqBDaFul"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Actividad libre (opcional si hay tiempo)\n",
        "\n",
        "Explorá uno de los pipelines y diseñá tu propio experimento:\n",
        "\n",
        "- Probá frases con sarcasmo o jergas locales.\n",
        "- Resumí un artículo de Wikipedia.\n",
        "- Traducí algo complejo (tecnológico, poético, etc.).\n",
        "- Completá una frase usando estilo formal o informal.\n",
        "\n",
        "Al final compartimos los hallazgos más interesantes con el grupo 👀\n",
        "\n"
      ],
      "metadata": {
        "id": "gC5dHmTaaOO7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Experimentos con Pipelines de Hugging Face"
      ],
      "metadata": {
        "id": "eiv-J84Lz-et"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importamos las librerías necesarias\n",
        "from transformers import pipeline\n"
      ],
      "metadata": {
        "id": "4isTKS20zJKi"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalación de Hugging Face Transformers\n",
        "\n",
        "print(\"Instalando Hugging Face Transformers...\")\n",
        "!pip install -q transformers\n",
        "print(\"Instalación completa.\")\n",
        "\n",
        "# Importamos la función 'pipeline' de la librería transformers.\n",
        "# La funcion nos permite usar modelos preentrenados para tareas específicas con muy pocas líneas de código.\n",
        "from transformers import pipeline\n",
        "print(\"Librería transformers y función pipeline importadas.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STr3X7Pj0HID",
        "outputId": "d653bd4b-469b-409f-890e-c187ee552a05"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Instalando Hugging Face Transformers...\n",
            "Instalación completa.\n",
            "Librería transformers y función pipeline importadas.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1° - Análisis de sentimiento con sarcasmo y jerga"
      ],
      "metadata": {
        "id": "mDVkIwDT0axn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargamos el pipeline de análisis de sentimiento con el modelo entrenado en español.\n",
        "# El modelo 'finiteautomata/beto-sentiment-analysis' está fine-tuneado para detectar sentimiento positivo/negativo.\n",
        "sentiment = pipeline(\"sentiment-analysis\", model=\"finiteautomata/beto-sentiment-analysis\")\n",
        "\n",
        "# Definimos una lista de frases que incluyen sarcasmo y jerga\n",
        "frases_experimentales = [\n",
        "    \"¡Qué bien! Justo lo que quería, que se rompa el ordenador.\",  # Sarcasmo\n",
        "    \"Sí, claro, soy millonario con este sueldo.\",              # Sarcasmo\n",
        "    \"La fiesta estuvo brutal, ¡lo pasamos de diez!\",          # Jerga (ej. España y otros lugares)\n",
        "    \"Ese chavo es muy buena onda.\",                            # Jerga (ej. México)\n",
        "    \"Me da pereza salir hoy.\",                                 # Jerga (ej. España)\n",
        "    \"Estoy re manija con este partido.\",                       # Jerga (ej. Argentina/Uruguay)\n",
        "    \"Es una ganga, casi me regalan el coche.\",                 # Sarcasmo\n",
        "    \"Tremendo quilombo armaron con el evento.\"                 # Jerga (ej. Argentina/Uruguay)\n",
        "]\n",
        "\n",
        "# Iteramos sobre cada frase y aplicamos el análisis de sentimiento\n",
        "print(\"--- Análisis de Sentimiento (Sarcasmo y Jerga) ---\")\n",
        "for frase in frases_experimentales:\n",
        "    # Aplicamos el pipeline a la frase. Devuelve una lista de diccionarios.\n",
        "    resultado = sentiment(frase)\n",
        "    # Imprimimos la frase original y el resultado del análisis (etiqueta y score)\n",
        "    print(f\"'{frase}' -> {resultado}\")\n",
        "    # Pausa opcional para facilitar la lectura\n",
        "    # import time\n",
        "    # time.sleep(0.5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMsJTrTa0iGw",
        "outputId": "52ced378-79c5-45de-d8ac-114d0cdf0466"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Análisis de Sentimiento (Sarcasmo y Jerga) ---\n",
            "'¡Qué bien! Justo lo que quería, que se rompa el ordenador.' -> [{'label': 'POS', 'score': 0.9816330671310425}]\n",
            "'Sí, claro, soy millonario con este sueldo.' -> [{'label': 'POS', 'score': 0.991821825504303}]\n",
            "'La fiesta estuvo brutal, ¡lo pasamos de diez!' -> [{'label': 'NEG', 'score': 0.9989134073257446}]\n",
            "'Ese chavo es muy buena onda.' -> [{'label': 'POS', 'score': 0.9987834095954895}]\n",
            "'Me da pereza salir hoy.' -> [{'label': 'NEG', 'score': 0.9993433356285095}]\n",
            "'Estoy re manija con este partido.' -> [{'label': 'POS', 'score': 0.9781509637832642}]\n",
            "'Es una ganga, casi me regalan el coche.' -> [{'label': 'POS', 'score': 0.9964009523391724}]\n",
            "'Tremendo quilombo armaron con el evento.' -> [{'label': 'NEG', 'score': 0.9992637038230896}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2° - Resumen de un fragmento de Wikipedia"
      ],
      "metadata": {
        "id": "qOCUMbzN0oPd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargamos el pipeline de resumen con el modelo multi-idioma.\n",
        "# 'csebuetnlp/mT5_multilingual_XLSum' es un modelo bueno para resumir en varios idiomas, incluyendo español.\n",
        "summarizer = pipeline(\n",
        "    \"summarization\",\n",
        "    model=\"csebuetnlp/mT5_multilingual_XLSum\",\n",
        "    tokenizer=\"csebuetnlp/mT5_multilingual_XLSum\"\n",
        ")\n",
        "\n",
        "# Fragmento de un artículo de Wikipedia (ejemplo: sobre la inteligencia artificial)\n",
        "texto_wikipedia = \"\"\"\n",
        "La inteligencia artificial (IA) es el campo de la ciencia de la computación dedicado a resolver problemas\n",
        "cognitivos comúnmente asociados a la inteligencia humana, como el aprendizaje, la resolución de problemas\n",
        "y el reconocimiento de patrones. La IA se divide en diferentes subcampos, como el aprendizaje automático,\n",
        "el procesamiento del lenguaje natural, la visión artificial y la robótica.\n",
        "El aprendizaje automático se centra en el desarrollo de algoritmos que permiten a las computadoras\n",
        "aprender de datos sin ser programadas explícitamente. Las redes neuronales son un tipo de algoritmo\n",
        "de aprendizaje automático que se inspira en la estructura del cerebro humano.\n",
        "El procesamiento del lenguaje natural (PLN) se ocupa de la interacción entre las computadoras y el lenguaje\n",
        "humano. Esto incluye tareas como la traducción automática, el análisis de sentimientos, la extracción de información\n",
        "y la generación de texto.\n",
        "La visión artificial permite a las computadoras \"ver\" e interpretar imágenes y videos. Se utiliza en\n",
        "aplicaciones como el reconocimiento facial, la detección de objetos y la conducción autónoma.\n",
        "La robótica se centra en la creación de robots capaces de realizar tareas de forma autónoma. A menudo\n",
        "combina elementos de los otros subcampos de la IA.\n",
        "La IA tiene un amplio rango de aplicaciones en diversos sectores, incluyendo la salud, las finanzas, la educación,\n",
        "el transporte y el entretenimiento. A pesar de sus avances, la IA también presenta desafíos éticos y sociales,\n",
        "como la privacidad de los datos, el sesgo algorítmico y el impacto en el empleo.\n",
        "\"\"\"\n",
        "\n",
        "# Aplicamos el pipeline de resumen. Experimentamos con la longitud máxima y mínima del resumen.\n",
        "print(\"\\n--- Resumen de Texto (Fragmento de Wikipedia) ---\")\n",
        "# max_length: número máximo de tokens en el resumen generado\n",
        "# min_length: número mínimo de tokens en el resumen generado\n",
        "# do_sample=False: Desactiva el muestreo para generar el resumen más probable\n",
        "resumen_generado = summarizer(texto_wikipedia, max_length=100, min_length=30, do_sample=False)\n",
        "\n",
        "# El resultado es una lista, extraemos el texto del resumen del primer elemento.\n",
        "print(resumen_generado[0]['summary_text'])\n",
        "\n",
        "# Puedes probar a cambiar los valores de max_length y min_length\n",
        "# resumen_generado_corto = summarizer(texto_wikipedia, max_length=50, min_length=15, do_sample=False)\n",
        "# print(\"\\n--- Resumen Corto ---\")\n",
        "# print(resumen_generado_corto[0]['summary_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "es8nR7wm0s6e",
        "outputId": "f1a063e1-0d10-403e-8905-51ac302a59e7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Resumen de Texto (Fragmento de Wikipedia) ---\n",
            "La inteligencia artificial (IA) es el campo de la computación dedicado a resolver problemas cognitivos comúnmente asociados al cerebro humano.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3° - Traducción de texto"
      ],
      "metadata": {
        "id": "cyydYqF_00yz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargamos el pipeline de traducción de español a inglés.\n",
        "# 'Helsinki-NLP/opus-mt-es-en' es un modelo entrenado por el grupo de investigación de Helsinki.\n",
        "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-es-en\")\n",
        "\n",
        "# Definimos una lista de textos para traducir, incluyendo ejemplos técnicos y posiblemente poéticos/figurados.\n",
        "textos_a_traducir = [\n",
        "    \"La arquitectura de red neuronal convolucional ha demostrado ser eficaz en tareas de visión artificial.\", # Técnico\n",
        "    \"El algoritmo de retropropagación ajusta los pesos de la red neuronal.\",                             # Técnico\n",
        "    \"En la vasta llanura del tiempo, las horas son gotas de rocío.\",                                     # Poético/Figurado\n",
        "    \"El alma que habla por los ojos, también puede besar con la mirada.\",                                 # Poético/Figurado (Gustavo Adolfo Bécquer)\n",
        "    \"Más vale pájaro en mano que ciento volando.\"                                                        # Refrán/Idioma\n",
        "]\n",
        "\n",
        "# Iteramos sobre cada texto y aplicamos la traducción\n",
        "print(\"\\n--- Traducción a Inglés ---\")\n",
        "for texto in textos_a_traducir:\n",
        "    # Aplicamos el pipeline de traducción\n",
        "    traduccion = translator(texto)\n",
        "    # Imprimimos el texto original y la traducción\n",
        "    print(f\"Original: '{texto}'\")\n",
        "    print(f\"Traducción: '{traduccion[0]['translation_text']}'\")\n",
        "    # Pausa opcional\n",
        "    # time.sleep(1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bNZX5aMY06VW",
        "outputId": "eeacc04f-55cc-47e1-934d-51d9f4040114"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Traducción a Inglés ---\n",
            "Original: 'La arquitectura de red neuronal convolucional ha demostrado ser eficaz en tareas de visión artificial.'\n",
            "Traducción: 'Convolutional neural network architecture has proven to be effective in artificial vision tasks.'\n",
            "Original: 'El algoritmo de retropropagación ajusta los pesos de la red neuronal.'\n",
            "Traducción: 'The retropropagation algorithm adjusts the weights of the neural network.'\n",
            "Original: 'En la vasta llanura del tiempo, las horas son gotas de rocío.'\n",
            "Traducción: 'In the vast plain of time, the hours are drops of dew.'\n",
            "Original: 'El alma que habla por los ojos, también puede besar con la mirada.'\n",
            "Traducción: 'The soul that speaks by the eyes, can also kiss with the look.'\n",
            "Original: 'Más vale pájaro en mano que ciento volando.'\n",
            "Traducción: 'Better bird in hand than a hundred flying.'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4° - Generación de texto con diferentes estilos"
      ],
      "metadata": {
        "id": "v3Ky_ZB61CSc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargamos el pipeline de generación de texto con un modelo tipo GPT en español.\n",
        "# 'PlanTL-GOB-ES/gpt2-base-bne' es un modelo GPT2 base fine-tuneado para español por el PlanTL-GOB-ES.\n",
        "generator = pipeline(\"text-generation\", model=\"PlanTL-GOB-ES/gpt2-base-bne\")\n",
        "\n",
        "# Definimos prompts iniciales que sugieren un estilo formal o informal\n",
        "prompts_estilo = [\n",
        "    \"Estimado/a colega,\",                       # Sugiere estilo formal\n",
        "    \"Estimados señores y señoras,\",             # Sugiere estilo formal\n",
        "    \"Hola,\",                                    # Sugiere estilo informal\n",
        "    \"Che, ¿cómo andás?\",                        # Sugiere estilo informal (jerga)\n",
        "    \"Buenas tardes,\"                            # Estilo neutral/semi-formal\n",
        "]\n",
        "\n",
        "print(\"\\n--- Generación de Texto (Intentando guiar estilo) ---\")\n",
        "for prompt in prompts_estilo:\n",
        "    print(f\"\\n--- Prompt: '{prompt}' ---\")\n",
        "    # Generamos texto a partir del prompt. Limitamos la longitud para que no sea demasiado largo.\n",
        "    # num_return_sequences=1: Generamos solo una secuencia por prompt.\n",
        "    resultado_generacion = generator(prompt, max_length=50, num_return_sequences=1)\n",
        "    # Imprimimos el texto generado.\n",
        "    print(resultado_generacion[0]['generated_text'])\n",
        "    # Pausa opcional\n",
        "    # time.sleep(1.5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "keufJzyF1Ffv",
        "outputId": "45efd267-9348-4b40-8d40-30879f24ed23"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=51) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Generación de Texto (Intentando guiar estilo) ---\n",
            "\n",
            "--- Prompt: 'Estimado/a colega,' ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=51) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estimado/a colega, no, no voy a entrar en lo duro, no voy a entrar en lo duro, no voy a entrar en lo duro, no voy a entrar en lo duro, no voy a entrar en lo duro, no voy a entrar en lo duro, no voy a entrar en lo duro, no voy a entrar en lo duro, no voy a entrar en lo duro, no voy a entrar en lo duro, no voy a entrar en lo duro, no voy a entrar en lo duro, no voy a entrar en lo duro, no voy a entrar en lo duro, no voy a entrar en lo duro, no voy a entrar en lo duro, no voy a entrar en lo duro, no voy a entrar en lo duro, no voy a entrar en lo duro, no voy a entrar en lo duro, no voy a entrar en lo duro, no voy a entrar en lo duro, no voy a entrar en lo duro, no voy a entrar en lo duro, no voy a entrar en lo duro, no voy a entrar en lo duro, no voy a entrar en lo duro, no voy a entrar en lo duro, no voy a entrar en lo duro, no voy a entrar en lo duro, no voy a entrar en lo duro, no voy a entrar en lo\n",
            "\n",
            "--- Prompt: 'Estimados señores y señoras,' ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=51) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estimados señores y señoras, \n",
            "\n",
            "--- Prompt: 'Hola,' ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=51) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hola, me puedes poner tu fecha, es decir, me. \n",
            "\n",
            "--- Prompt: 'Che, ¿cómo andás?' ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=51) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Che, ¿cómo andás? bueno, ya sé que en la vida hay muchos que no entienden lo que es la fama, pero... \n",
            "\n",
            "--- Prompt: 'Buenas tardes,' ---\n",
            "Buenas tardes, ¿Qué tal? \n"
          ]
        }
      ]
    }
  ]
}