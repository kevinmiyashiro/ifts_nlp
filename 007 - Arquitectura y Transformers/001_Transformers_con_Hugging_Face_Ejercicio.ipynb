{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Reflexi√≥n y discusi√≥n\n",
        "\n",
        "- ¬øCu√°l de estos pipelines te pareci√≥ m√°s sorprendente o √∫til?\n",
        "- ¬øCre√©s que estas herramientas podr√≠an usarse en un proyecto real? ¬øEn cu√°l?\n",
        "- ¬øNotaste errores o sesgos? ¬øPor qu√© cre√©s que aparecen?\n",
        "\n"
      ],
      "metadata": {
        "id": "fH5RSqBDaFul"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Actividad libre (opcional si hay tiempo)\n",
        "\n",
        "Explor√° uno de los pipelines y dise√±√° tu propio experimento:\n",
        "\n",
        "- Prob√° frases con sarcasmo o jergas locales.\n",
        "- Resum√≠ un art√≠culo de Wikipedia.\n",
        "- Traduc√≠ algo complejo (tecnol√≥gico, po√©tico, etc.).\n",
        "- Complet√° una frase usando estilo formal o informal.\n",
        "\n",
        "Al final compartimos los hallazgos m√°s interesantes con el grupo üëÄ\n",
        "\n"
      ],
      "metadata": {
        "id": "gC5dHmTaaOO7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Experimentos con Pipelines de Hugging Face"
      ],
      "metadata": {
        "id": "eiv-J84Lz-et"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importamos las librer√≠as necesarias\n",
        "from transformers import pipeline\n"
      ],
      "metadata": {
        "id": "4isTKS20zJKi"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalaci√≥n de Hugging Face Transformers\n",
        "\n",
        "print(\"Instalando Hugging Face Transformers...\")\n",
        "!pip install -q transformers\n",
        "print(\"Instalaci√≥n completa.\")\n",
        "\n",
        "# Importamos la funci√≥n 'pipeline' de la librer√≠a transformers.\n",
        "# La funcion nos permite usar modelos preentrenados para tareas espec√≠ficas con muy pocas l√≠neas de c√≥digo.\n",
        "from transformers import pipeline\n",
        "print(\"Librer√≠a transformers y funci√≥n pipeline importadas.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STr3X7Pj0HID",
        "outputId": "d653bd4b-469b-409f-890e-c187ee552a05"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Instalando Hugging Face Transformers...\n",
            "Instalaci√≥n completa.\n",
            "Librer√≠a transformers y funci√≥n pipeline importadas.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1¬∞ - An√°lisis de sentimiento con sarcasmo y jerga"
      ],
      "metadata": {
        "id": "mDVkIwDT0axn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargamos el pipeline de an√°lisis de sentimiento con el modelo entrenado en espa√±ol.\n",
        "# El modelo 'finiteautomata/beto-sentiment-analysis' est√° fine-tuneado para detectar sentimiento positivo/negativo.\n",
        "sentiment = pipeline(\"sentiment-analysis\", model=\"finiteautomata/beto-sentiment-analysis\")\n",
        "\n",
        "# Definimos una lista de frases que incluyen sarcasmo y jerga\n",
        "frases_experimentales = [\n",
        "    \"¬°Qu√© bien! Justo lo que quer√≠a, que se rompa el ordenador.\",  # Sarcasmo\n",
        "    \"S√≠, claro, soy millonario con este sueldo.\",              # Sarcasmo\n",
        "    \"La fiesta estuvo brutal, ¬°lo pasamos de diez!\",          # Jerga (ej. Espa√±a y otros lugares)\n",
        "    \"Ese chavo es muy buena onda.\",                            # Jerga (ej. M√©xico)\n",
        "    \"Me da pereza salir hoy.\",                                 # Jerga (ej. Espa√±a)\n",
        "    \"Estoy re manija con este partido.\",                       # Jerga (ej. Argentina/Uruguay)\n",
        "    \"Es una ganga, casi me regalan el coche.\",                 # Sarcasmo\n",
        "    \"Tremendo quilombo armaron con el evento.\"                 # Jerga (ej. Argentina/Uruguay)\n",
        "]\n",
        "\n",
        "# Iteramos sobre cada frase y aplicamos el an√°lisis de sentimiento\n",
        "print(\"--- An√°lisis de Sentimiento (Sarcasmo y Jerga) ---\")\n",
        "for frase in frases_experimentales:\n",
        "    # Aplicamos el pipeline a la frase. Devuelve una lista de diccionarios.\n",
        "    resultado = sentiment(frase)\n",
        "    # Imprimimos la frase original y el resultado del an√°lisis (etiqueta y score)\n",
        "    print(f\"'{frase}' -> {resultado}\")\n",
        "    # Pausa opcional para facilitar la lectura\n",
        "    # import time\n",
        "    # time.sleep(0.5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMsJTrTa0iGw",
        "outputId": "52ced378-79c5-45de-d8ac-114d0cdf0466"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- An√°lisis de Sentimiento (Sarcasmo y Jerga) ---\n",
            "'¬°Qu√© bien! Justo lo que quer√≠a, que se rompa el ordenador.' -> [{'label': 'POS', 'score': 0.9816330671310425}]\n",
            "'S√≠, claro, soy millonario con este sueldo.' -> [{'label': 'POS', 'score': 0.991821825504303}]\n",
            "'La fiesta estuvo brutal, ¬°lo pasamos de diez!' -> [{'label': 'NEG', 'score': 0.9989134073257446}]\n",
            "'Ese chavo es muy buena onda.' -> [{'label': 'POS', 'score': 0.9987834095954895}]\n",
            "'Me da pereza salir hoy.' -> [{'label': 'NEG', 'score': 0.9993433356285095}]\n",
            "'Estoy re manija con este partido.' -> [{'label': 'POS', 'score': 0.9781509637832642}]\n",
            "'Es una ganga, casi me regalan el coche.' -> [{'label': 'POS', 'score': 0.9964009523391724}]\n",
            "'Tremendo quilombo armaron con el evento.' -> [{'label': 'NEG', 'score': 0.9992637038230896}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2¬∞ - Resumen de un fragmento de Wikipedia"
      ],
      "metadata": {
        "id": "qOCUMbzN0oPd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargamos el pipeline de resumen con el modelo multi-idioma.\n",
        "# 'csebuetnlp/mT5_multilingual_XLSum' es un modelo bueno para resumir en varios idiomas, incluyendo espa√±ol.\n",
        "summarizer = pipeline(\n",
        "    \"summarization\",\n",
        "    model=\"csebuetnlp/mT5_multilingual_XLSum\",\n",
        "    tokenizer=\"csebuetnlp/mT5_multilingual_XLSum\"\n",
        ")\n",
        "\n",
        "# Fragmento de un art√≠culo de Wikipedia (ejemplo: sobre la inteligencia artificial)\n",
        "texto_wikipedia = \"\"\"\n",
        "La inteligencia artificial (IA) es el campo de la ciencia de la computaci√≥n dedicado a resolver problemas\n",
        "cognitivos com√∫nmente asociados a la inteligencia humana, como el aprendizaje, la resoluci√≥n de problemas\n",
        "y el reconocimiento de patrones. La IA se divide en diferentes subcampos, como el aprendizaje autom√°tico,\n",
        "el procesamiento del lenguaje natural, la visi√≥n artificial y la rob√≥tica.\n",
        "El aprendizaje autom√°tico se centra en el desarrollo de algoritmos que permiten a las computadoras\n",
        "aprender de datos sin ser programadas expl√≠citamente. Las redes neuronales son un tipo de algoritmo\n",
        "de aprendizaje autom√°tico que se inspira en la estructura del cerebro humano.\n",
        "El procesamiento del lenguaje natural (PLN) se ocupa de la interacci√≥n entre las computadoras y el lenguaje\n",
        "humano. Esto incluye tareas como la traducci√≥n autom√°tica, el an√°lisis de sentimientos, la extracci√≥n de informaci√≥n\n",
        "y la generaci√≥n de texto.\n",
        "La visi√≥n artificial permite a las computadoras \"ver\" e interpretar im√°genes y videos. Se utiliza en\n",
        "aplicaciones como el reconocimiento facial, la detecci√≥n de objetos y la conducci√≥n aut√≥noma.\n",
        "La rob√≥tica se centra en la creaci√≥n de robots capaces de realizar tareas de forma aut√≥noma. A menudo\n",
        "combina elementos de los otros subcampos de la IA.\n",
        "La IA tiene un amplio rango de aplicaciones en diversos sectores, incluyendo la salud, las finanzas, la educaci√≥n,\n",
        "el transporte y el entretenimiento. A pesar de sus avances, la IA tambi√©n presenta desaf√≠os √©ticos y sociales,\n",
        "como la privacidad de los datos, el sesgo algor√≠tmico y el impacto en el empleo.\n",
        "\"\"\"\n",
        "\n",
        "# Aplicamos el pipeline de resumen. Experimentamos con la longitud m√°xima y m√≠nima del resumen.\n",
        "print(\"\\n--- Resumen de Texto (Fragmento de Wikipedia) ---\")\n",
        "# max_length: n√∫mero m√°ximo de tokens en el resumen generado\n",
        "# min_length: n√∫mero m√≠nimo de tokens en el resumen generado\n",
        "# do_sample=False: Desactiva el muestreo para generar el resumen m√°s probable\n",
        "resumen_generado = summarizer(texto_wikipedia, max_length=100, min_length=30, do_sample=False)\n",
        "\n",
        "# El resultado es una lista, extraemos el texto del resumen del primer elemento.\n",
        "print(resumen_generado[0]['summary_text'])\n",
        "\n",
        "# Puedes probar a cambiar los valores de max_length y min_length\n",
        "# resumen_generado_corto = summarizer(texto_wikipedia, max_length=50, min_length=15, do_sample=False)\n",
        "# print(\"\\n--- Resumen Corto ---\")\n",
        "# print(resumen_generado_corto[0]['summary_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "es8nR7wm0s6e",
        "outputId": "f1a063e1-0d10-403e-8905-51ac302a59e7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Resumen de Texto (Fragmento de Wikipedia) ---\n",
            "La inteligencia artificial (IA) es el campo de la computaci√≥n dedicado a resolver problemas cognitivos com√∫nmente asociados al cerebro humano.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3¬∞ - Traducci√≥n de texto"
      ],
      "metadata": {
        "id": "cyydYqF_00yz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargamos el pipeline de traducci√≥n de espa√±ol a ingl√©s.\n",
        "# 'Helsinki-NLP/opus-mt-es-en' es un modelo entrenado por el grupo de investigaci√≥n de Helsinki.\n",
        "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-es-en\")\n",
        "\n",
        "# Definimos una lista de textos para traducir, incluyendo ejemplos t√©cnicos y posiblemente po√©ticos/figurados.\n",
        "textos_a_traducir = [\n",
        "    \"La arquitectura de red neuronal convolucional ha demostrado ser eficaz en tareas de visi√≥n artificial.\", # T√©cnico\n",
        "    \"El algoritmo de retropropagaci√≥n ajusta los pesos de la red neuronal.\",                             # T√©cnico\n",
        "    \"En la vasta llanura del tiempo, las horas son gotas de roc√≠o.\",                                     # Po√©tico/Figurado\n",
        "    \"El alma que habla por los ojos, tambi√©n puede besar con la mirada.\",                                 # Po√©tico/Figurado (Gustavo Adolfo B√©cquer)\n",
        "    \"M√°s vale p√°jaro en mano que ciento volando.\"                                                        # Refr√°n/Idioma\n",
        "]\n",
        "\n",
        "# Iteramos sobre cada texto y aplicamos la traducci√≥n\n",
        "print(\"\\n--- Traducci√≥n a Ingl√©s ---\")\n",
        "for texto in textos_a_traducir:\n",
        "    # Aplicamos el pipeline de traducci√≥n\n",
        "    traduccion = translator(texto)\n",
        "    # Imprimimos el texto original y la traducci√≥n\n",
        "    print(f\"Original: '{texto}'\")\n",
        "    print(f\"Traducci√≥n: '{traduccion[0]['translation_text']}'\")\n",
        "    # Pausa opcional\n",
        "    # time.sleep(1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bNZX5aMY06VW",
        "outputId": "eeacc04f-55cc-47e1-934d-51d9f4040114"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Traducci√≥n a Ingl√©s ---\n",
            "Original: 'La arquitectura de red neuronal convolucional ha demostrado ser eficaz en tareas de visi√≥n artificial.'\n",
            "Traducci√≥n: 'Convolutional neural network architecture has proven to be effective in artificial vision tasks.'\n",
            "Original: 'El algoritmo de retropropagaci√≥n ajusta los pesos de la red neuronal.'\n",
            "Traducci√≥n: 'The retropropagation algorithm adjusts the weights of the neural network.'\n",
            "Original: 'En la vasta llanura del tiempo, las horas son gotas de roc√≠o.'\n",
            "Traducci√≥n: 'In the vast plain of time, the hours are drops of dew.'\n",
            "Original: 'El alma que habla por los ojos, tambi√©n puede besar con la mirada.'\n",
            "Traducci√≥n: 'The soul that speaks by the eyes, can also kiss with the look.'\n",
            "Original: 'M√°s vale p√°jaro en mano que ciento volando.'\n",
            "Traducci√≥n: 'Better bird in hand than a hundred flying.'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4¬∞ - Generaci√≥n de texto con diferentes estilos"
      ],
      "metadata": {
        "id": "v3Ky_ZB61CSc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargamos el pipeline de generaci√≥n de texto con un modelo tipo GPT en espa√±ol.\n",
        "# 'PlanTL-GOB-ES/gpt2-base-bne' es un modelo GPT2 base fine-tuneado para espa√±ol por el PlanTL-GOB-ES.\n",
        "generator = pipeline(\"text-generation\", model=\"PlanTL-GOB-ES/gpt2-base-bne\")\n",
        "\n",
        "# Definimos prompts iniciales que sugieren un estilo formal o informal\n",
        "prompts_estilo = [\n",
        "    \"Estimado/a colega,\",                       # Sugiere estilo formal\n",
        "    \"Estimados se√±ores y se√±oras,\",             # Sugiere estilo formal\n",
        "    \"Hola,\",                                    # Sugiere estilo informal\n",
        "    \"Che, ¬øc√≥mo and√°s?\",                        # Sugiere estilo informal (jerga)\n",
        "    \"Buenas tardes,\"                            # Estilo neutral/semi-formal\n",
        "]\n",
        "\n",
        "print(\"\\n--- Generaci√≥n de Texto (Intentando guiar estilo) ---\")\n",
        "for prompt in prompts_estilo:\n",
        "    print(f\"\\n--- Prompt: '{prompt}' ---\")\n",
        "    # Generamos texto a partir del prompt. Limitamos la longitud para que no sea demasiado largo.\n",
        "    # num_return_sequences=1: Generamos solo una secuencia por prompt.\n",
        "    resultado_generacion = generator(prompt, max_length=50, num_return_sequences=1)\n",
        "    # Imprimimos el texto generado.\n",
        "    print(resultado_generacion[0]['generated_text'])\n",
        "    # Pausa opcional\n",
        "    # time.sleep(1.5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "keufJzyF1Ffv",
        "outputId": "45efd267-9348-4b40-8d40-30879f24ed23"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=51) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Generaci√≥n de Texto (Intentando guiar estilo) ---\n",
            "\n",
            "--- Prompt: 'Estimado/a colega,' ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=51) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estimado/a colega, no, no voy a entrar en lo duro, no voy a entrar en lo duro, no voy a entrar en lo duro, no voy a entrar en lo duro, no voy a entrar en lo duro, no voy a entrar en lo duro, no voy a entrar en lo duro, no voy a entrar en lo duro, no voy a entrar en lo duro, no voy a entrar en lo duro, no voy a entrar en lo duro, no voy a entrar en lo duro, no voy a entrar en lo duro, no voy a entrar en lo duro, no voy a entrar en lo duro, no voy a entrar en lo duro, no voy a entrar en lo duro, no voy a entrar en lo duro, no voy a entrar en lo duro, no voy a entrar en lo duro, no voy a entrar en lo duro, no voy a entrar en lo duro, no voy a entrar en lo duro, no voy a entrar en lo duro, no voy a entrar en lo duro, no voy a entrar en lo duro, no voy a entrar en lo duro, no voy a entrar en lo duro, no voy a entrar en lo duro, no voy a entrar en lo duro, no voy a entrar en lo duro, no voy a entrar en lo\n",
            "\n",
            "--- Prompt: 'Estimados se√±ores y se√±oras,' ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=51) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estimados se√±ores y se√±oras, \n",
            "\n",
            "--- Prompt: 'Hola,' ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=51) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hola, me puedes poner tu fecha, es decir, me. \n",
            "\n",
            "--- Prompt: 'Che, ¬øc√≥mo and√°s?' ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=51) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Che, ¬øc√≥mo and√°s? bueno, ya s√© que en la vida hay muchos que no entienden lo que es la fama, pero... \n",
            "\n",
            "--- Prompt: 'Buenas tardes,' ---\n",
            "Buenas tardes, ¬øQu√© tal? \n"
          ]
        }
      ]
    }
  ]
}