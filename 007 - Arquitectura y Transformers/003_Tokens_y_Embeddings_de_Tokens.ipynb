{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "üí° **NOTA**: Necesitaremos usar una GPU para ejecutar los ejemplos en este cuaderno. En Google Colab, and√° a\n",
        "**Runtime > Change runtime type > Hardware accelerator > GPU > GPU type > T4**.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "G3AKzqeOJ_uV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install --upgrade pip\n",
        "!pip uninstall -y transformers accelerate\n",
        "!pip install transformers accelerate\n",
        "!pip install sentence-transformers gensim scikit-learn numpy scipy"
      ],
      "metadata": {
        "id": "nMmqo5QSBJp6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Descargando y Ejecutando un LLM\n",
        "\n",
        "El primer paso es cargar nuestro modelo en la GPU para una inferencia m√°s r√°pida. Ten√© en cuenta que cargamos el modelo y el tokenizador por separado para poder explorarlos individualmente."
      ],
      "metadata": {
        "id": "2Q-4QJAnKI5a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Cargar modelo y tokenizador\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
        "    device_map=\"cpu\",\n",
        "    torch_dtype=\"auto\",\n",
        "    trust_remote_code=False,\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")"
      ],
      "metadata": {
        "id": "e6Jl871hZ4oh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejemplo en espa√±ol\n",
        "prompt = \"Escrib√≠ un email disculp√°ndote con Mar√≠a por el incidente en el asado del domingo. Explic√° c√≥mo sucedi√≥.<|assistant|>\"\n",
        "\n",
        "# Tokenizar el prompt de entrada\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cpu\")\n"
      ],
      "metadata": {
        "id": "YNfXkEDHOFiE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generar el texto\n",
        "generation_output = model.generate(\n",
        "  input_ids=input_ids,\n",
        "  max_new_tokens=200\n",
        ")\n",
        "\n",
        "# Imprimir la salida\n",
        "print(tokenizer.decode(generation_output[0]))"
      ],
      "metadata": {
        "id": "Y-mnMhycPkZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generar el texto\n",
        "generation_output = model.generate(\n",
        "  input_ids=input_ids,\n",
        "  max_new_tokens=100\n",
        ")\n",
        "\n",
        "# Imprimir la salida\n",
        "print(tokenizer.decode(generation_output[0]))"
      ],
      "metadata": {
        "id": "9If-CgrqPltS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparando Tokenizadores de LLMs Entrenados\n",
        "\n",
        "En esta secci√≥n, vamos a ver c√≥mo diferentes modelos tokenizan el texto. Es interesante ver c√≥mo cada modelo maneja el espa√±ol, los emojis y diferentes tipos de texto."
      ],
      "metadata": {
        "id": "RWkFZcASKqBs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "colors_list = [\n",
        "    '102;194;165', '252;141;98', '141;160;203',\n",
        "    '231;138;195', '166;216;84', '255;217;47'\n",
        "]\n",
        "\n",
        "def show_tokens(sentence, tokenizer_name):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
        "    token_ids = tokenizer(sentence).input_ids\n",
        "    for idx, t in enumerate(token_ids):\n",
        "        print(\n",
        "            f'\\x1b[0;30;48;2;{colors_list[idx % len(colors_list)]}m' +\n",
        "            tokenizer.decode(t) +\n",
        "            '\\x1b[0m',\n",
        "            end=' '\n",
        "        )"
      ],
      "metadata": {
        "id": "5b2dRAWvKuSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Texto de ejemplo adaptado al espa√±ol y contexto argentino\n",
        "text = \"\"\"\n",
        "Espa√±ol y MAY√öSCULAS\n",
        "üßâ √±\n",
        "show_tokens False None elif == >= else: dos tabs:\"    \" Tres tabs: \"       \"\n",
        "12.0*50=600\n",
        "El mate est√° muy caliente\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "tKg5nqjMLoVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Probamos diferentes tokenizadores\n",
        "show_tokens(text, \"bert-base-uncased\")"
      ],
      "metadata": {
        "id": "1y5_WltpLqsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_tokens(text, \"bert-base-cased\")"
      ],
      "metadata": {
        "id": "OoU2hJy-LvHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_tokens(text, \"microsoft/Phi-3-mini-4k-instruct\")"
      ],
      "metadata": {
        "id": "OFpE_p5uLwlZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_tokens(text, \"gpt2\")"
      ],
      "metadata": {
        "id": "I4defet1LzRE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embeddings de Palabras Contextualizados desde un Modelo de Lenguaje (Como BERT)\n",
        "\n",
        "Los embeddings contextualizados nos permiten obtener representaciones num√©ricas de palabras que tienen en cuenta el contexto en el que aparecen."
      ],
      "metadata": {
        "id": "YFiyKFaLK70D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "# Cargar tokenizador\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-base\")\n",
        "\n",
        "# Cargar modelo de lenguaje\n",
        "model = AutoModel.from_pretrained(\"microsoft/deberta-v3-xsmall\")\n",
        "\n",
        "# Tokenizar la oraci√≥n\n",
        "tokens = tokenizer('Hola mundo', return_tensors='pt')\n",
        "\n",
        "# Procesar los tokens\n",
        "output = model(**tokens)[0]\n",
        "\n",
        "print(\"Forma del tensor de salida:\")\n",
        "print(output.shape)\n",
        "\n",
        "print(\"\\nTokens individuales:\")\n",
        "for token in tokens['input_ids'][0]:\n",
        "    print(tokenizer.decode(token))"
      ],
      "metadata": {
        "id": "e27t3BsbLBaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embeddings de Texto (Para Oraciones y Documentos Completos)\n",
        "\n",
        "Los embeddings de texto nos permiten convertir oraciones completas en vectores num√©ricos, √∫tiles para b√∫squeda sem√°ntica y an√°lisis de similitud."
      ],
      "metadata": {
        "id": "b_v_R8yULIqu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Cargar modelo\n",
        "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')"
      ],
      "metadata": {
        "id": "cDjB0TeLLLEt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convertir texto a embeddings\n",
        "vector = model.encode(\"¬°La mejor pel√≠cula que vi!\")\n",
        "\n",
        "print(\"Dimensiones del vector:\", vector.shape)"
      ],
      "metadata": {
        "id": "pWNc2dyAXaS0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}